{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction To TF-IDF \n",
    "It is a technic called Term Frequency - Inversed Document Frequency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF\n",
    "\n",
    "### Definition\n",
    "Simply calculates the term frequency to check how frequently a terms occurs in a document. \n",
    "If a term occurs more times in a document, then the term has more relevance than other terms.\n",
    "\n",
    "### Formula \n",
    "    t : term , d : document\n",
    "    tf(t,d) = count of t in d / number of words in d\n",
    "### Example \n",
    "I love naturla language processing:\n",
    "\n",
    "| Sentence | TF Score |\n",
    "| --- | --- |\n",
    "| I | 1/5 |  \n",
    "| love | 1/5 |\n",
    "| natural | 1/5 |\n",
    "| language | 1/5 |\n",
    "| processing | 1/5 |\n",
    "\n",
    "Another Example : \n",
    "I have a job, I love my job. My job is cool:\n",
    "\n",
    "| Sentence | TF Score |\n",
    "| --- | --- |\n",
    "| I | 2/12 |  \n",
    "| have | 1/12 |\n",
    "| a | 1/12 |\n",
    "| job | 3/12 |\n",
    "| love | 1/12 |\n",
    "| my | 2/12 |\n",
    "| is | 1/12 |\n",
    "| cool | 1/12 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF \n",
    "While computing TF, all terms are considered equally important. However it is known that certain terms, such as “is”, “of”, and “that”, may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing IDF, an inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.\n",
    "\n",
    "IDF is the inverse of the document frequency which measures the informativeness of term t. When we calculate IDF, it will be very low for the most occurring words such as stop words (because stop words such as “is” is present in almost all of the documents, and N/df will give a very low value to that word). This finally gives what we want, a relative weightage.\n",
    "\n",
    "formula : idf(t) = N/df\n",
    "Now there are few other problems with the IDF , in case of a large corpus,say 100,000,000 , the IDF value explodes , to avoid the effect we take the log of idf .\n",
    "\n",
    "During the query time, when a word which is not in vocab occurs, the df will be 0. As we cannot divide by 0, we smoothen the value by adding 1 to the denominator.\n",
    "\n",
    "that’s the final formula:\n",
    "### Formula : \n",
    "    idf(t) = log(N/(df + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'natural', 'in', 'language', 'I', 'have', 'job', 'modeling', 'a', 'processing', 'that', 'love'}\n"
     ]
    }
   ],
   "source": [
    "first_sentence = \"I love natural language processing\"\n",
    "second_sentence = \"I have a job in language modeling that I love\"\n",
    "\n",
    "#split so each word have their own string\n",
    "first_sentence = first_sentence.split(\" \")\n",
    "second_sentence = second_sentence.split(\" \")#join them to remove common duplicate words\n",
    "total= set(first_sentence).union(set(second_sentence))\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDictA = dict.fromkeys(total, 0) \n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "for word in first_sentence:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in second_sentence:\n",
    "    wordDictB[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>a</th>\n",
       "      <th>have</th>\n",
       "      <th>in</th>\n",
       "      <th>job</th>\n",
       "      <th>language</th>\n",
       "      <th>love</th>\n",
       "      <th>modeling</th>\n",
       "      <th>natural</th>\n",
       "      <th>processing</th>\n",
       "      <th>that</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   I  a  have  in  job  language  love  modeling  natural  processing  that\n",
       "0  1  0     0   0    0         1     1         0        1           1     0\n",
       "1  2  1     1   1    1         1     1         1        0           0     1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s all for TF formula , just i wanna talk about stop words that we should eliminate them because they are the most commonly occurring words which don’t give any additional value to the document vector .in-fact removing these will increase computation and space efficiency.\n",
    "\n",
    "nltk library has a method to download the stopwords, so instead of explicitly mentioning all the stopwords ourselves we can just use the nltk library and iterate over all the words and remove the stop words. There are many efficient ways to do this, but ill just give a simple method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)\n",
    "#running our sentences through the tf function:\n",
    "tfFirst = computeTF(wordDictA, first_sentence)\n",
    "tfSecond = computeTF(wordDictB, second_sentence)\n",
    "#Converting to dataframe for visualization\n",
    "tf = pd.DataFrame([tfFirst, tfSecond])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natural': 0.09090909090909091,\n",
       " 'in': 0.0,\n",
       " 'language': 0.09090909090909091,\n",
       " 'I': 0.09090909090909091,\n",
       " 'have': 0.0,\n",
       " 'job': 0.0,\n",
       " 'modeling': 0.0,\n",
       " 'a': 0.0,\n",
       " 'processing': 0.09090909090909091,\n",
       " 'that': 0.0,\n",
       " 'love': 0.09090909090909091}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeTF(wordDictA,wordDictB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StopWords with NLTK \n",
    "That’s all for TF formula , just i wanna talk about stop words that we should eliminate them because they are the most commonly occurring words which don’t give any additional value to the document vector .in-fact removing these will increase computation and space efficiency.\n",
    "\n",
    "nltk library has a method to download the stopwords, so instead of explicitly mentioning all the stopwords ourselves we can just use the nltk library and iterate over all the words and remove the stop words. There are many efficient ways to do this, but ill just give a simple method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/wael/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "['natural', 'language', 'I', 'job', 'modeling', 'processing', 'love']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_sentence = [w for w in wordDictA if not w in stop_words]\n",
    "print (filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / (float(val) + 1))\n",
    "        \n",
    "    return(idfDict)\n",
    "#inputing our sentences in the log file\n",
    "\n",
    "idfs = computeIDF([wordDictA, wordDictB])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now we implement the idf formula , let’s finish with calculating the TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return(tfidf)\n",
    "#running our two sentences through the IDF:\n",
    "idfFirst = computeTFIDF(tfFirst, idfs)\n",
    "idfSecond = computeTFIDF(tfSecond, idfs)\n",
    "#putting it in a dataframe\n",
    "idf= pd.DataFrame([idfFirst, idfSecond])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a lot of work. But it is handy to know, if you are asked to code TF-IDF from scratch in the future. However, this can be done a lot simpler thanks to sklearn library. Let’s look at the example from them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first step is to import the library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#for the sentence, make sure all words are lowercase or you will run #into error. for simplicity, I just made the same sentence all #lowercase\n",
    "firstV= \"Data Science is the sexiest job of the 21st century\"\n",
    "secondV= \"machine learning is the key for data science\"\n",
    "#calling the TfidfVectorizer\n",
    "vectorize= TfidfVectorizer()\n",
    "#fitting the model and passing our sentences right away:\n",
    "response= vectorize.fit_transform([firstV, secondV])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.24342026926924518\n",
      "  (0, 10)\t0.24342026926924518\n",
      "  (0, 4)\t0.24342026926924518\n",
      "  (0, 12)\t0.48684053853849035\n",
      "  (0, 11)\t0.34211869506421816\n",
      "  (0, 5)\t0.34211869506421816\n",
      "  (0, 9)\t0.34211869506421816\n",
      "  (0, 0)\t0.34211869506421816\n",
      "  (0, 1)\t0.34211869506421816\n",
      "  (1, 2)\t0.28986933576883284\n",
      "  (1, 10)\t0.28986933576883284\n",
      "  (1, 4)\t0.28986933576883284\n",
      "  (1, 12)\t0.28986933576883284\n",
      "  (1, 8)\t0.40740123733358447\n",
      "  (1, 7)\t0.40740123733358447\n",
      "  (1, 6)\t0.40740123733358447\n",
      "  (1, 3)\t0.40740123733358447\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfA = computeTF(wordDictA, first_sentence)\n",
    "tfB = computeTF(wordDictB, second_sentence)\n",
    "\n",
    "idfs = computeIDF([wordDictA, wordDictB])\n",
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "\n",
    "#res = computeTFIDF([firstV, secondV])\n",
    "df = pd.DataFrame([tfidfA, tfidfB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>a</th>\n",
       "      <th>have</th>\n",
       "      <th>in</th>\n",
       "      <th>job</th>\n",
       "      <th>language</th>\n",
       "      <th>love</th>\n",
       "      <th>modeling</th>\n",
       "      <th>natural</th>\n",
       "      <th>processing</th>\n",
       "      <th>that</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.060206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060206</td>\n",
       "      <td>0.060206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060206</td>\n",
       "      <td>0.060206</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.060206</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          I         a      have        in       job  language      love  \\\n",
       "0  0.060206  0.000000  0.000000  0.000000  0.000000  0.060206  0.060206   \n",
       "1  0.060206  0.030103  0.030103  0.030103  0.030103  0.030103  0.030103   \n",
       "\n",
       "   modeling   natural  processing      that  \n",
       "0  0.000000  0.060206    0.060206  0.000000  \n",
       "1  0.030103  0.000000    0.000000  0.030103  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
